{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "615b26e0-547d-4ae2-bd09-bf7c81a70970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==3.1.0\n",
    "#If you issues, make sure this matches your automl dependency version. For prod usage, use env_manager='conda'\n",
    "%pip install azure-core azure-storage-file-datalake #for the display() in Azure only\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "0563fced-61d3-4fc0-927e-fa45587003b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data engineering with Databricks - Building our C360 database\n",
    "\n",
    "Building a C360 database requires to ingest multiple datasources.  \n",
    "\n",
    "It's a complex process requiring batch loads and streaming ingestion to support real-time insights, used for personalization and marketing targeting among other.\n",
    "\n",
    "Ingesting, transforming and cleaning data to create clean SQL tables for our downstream user (Data Analysts and Data Scientists) is complex.\n",
    "\n",
    "<link href=\"https://fonts.googleapis.com/css?family=DM Sans\" rel=\"stylesheet\"/>\n",
    "<div style=\"width: 300px; height: 300px; text-align: center; float: right; margin: 30px 60px 10px 10px; font-family: 'DM Sans'; border-radius: 50%; border: 25px solid #fcba33ff; box-sizing: border-box; overflow: hidden;\">\n",
    "  <div style=\"display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100%; width: 100%;\">\n",
    "    <div style=\"font-size: 70px; color: #70c4ab; font-weight: bold;\">\n",
    "      73%\n",
    "    </div>\n",
    "    <div style=\"color: #1b5162; padding: 0 30px; text-align: center;\">\n",
    "      of enterprise data goes unused for analytics and decision making\n",
    "    </div>\n",
    "  </div>\n",
    "  <div style=\"color: #bfbfbf; padding-top: 5px;\">\n",
    "    Source: Forrester\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## <img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/john.png\" style=\"float:left; margin: -35px 0px 0px 0px\" width=\"80px\"> John, as Data engineer, spends immense timeâ€¦.\n",
    "\n",
    "* Hand-coding data ingestion & transformations and dealing with technical challenges:<br>\n",
    "  *Supporting streaming and batch, handling concurrent operations, small files issues, GDPR requirements, complex DAG dependencies...*<br><br>\n",
    "* Building custom frameworks to enforce quality and tests<br><br>\n",
    "* Building and maintaining scalable infrastructure, with observability and monitoring<br><br>\n",
    "* Managing incompatible governance models from different systems\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "This results in **operational complexity** and overhead, requiring expert profile and ultimatly **putting data projects at risk**.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=1444828305810485&notebook=%2F01-Data-ingestion%2F01.3-DLT-churn-python&demo_name=lakehouse-retail-c360&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-retail-c360%2F01-Data-ingestion%2F01.3-DLT-churn-python&version=1&user_hash=0b3be070fa39374fb760232ebb606a5c489732ec881a7ebfc68231c496aed118\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "303673c8-1f16-4cef-abee-618eade4656a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simplify Ingestion and Transformation with Lakeflow Connect & DLT\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/cross_demo_assets/Lakehouse_Demo_Team_architecture_1.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "In this notebook, we'll work as a Data Engineer to build our c360 database. <br>\n",
    "We'll consume and clean our raw data sources to prepare the tables required for our BI & ML workload.\n",
    "\n",
    "We want to ingest the datasets below from Salesforce Sales Cloud and blob storage (`/demos/retail/churn/`) incrementally into our Data Warehousing tables:\n",
    "\n",
    "- Customer profile data *(name, age, address etc)*\n",
    "- Orders history *(what our customer bought over time)*\n",
    "- Streaming Events from our application *(when was the last time customers used the application, typically a stream from a Kafka queue)*\n",
    "\n",
    "\n",
    "<a href=\"https://www.databricks.com/resources/demos/tours/platform/discover-databricks-lakeflow-connect-demo\" target=\"_blank\"><img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/lakeflow-connect-anim.gif?raw=true\" style=\"float: right; margin-right: 20px\" width=\"250px\"></a>\n",
    "\n",
    "## 1/ Ingest data with Lakeflow Connect\n",
    "\n",
    "\n",
    "Lakeflow Connect offers built-in data ingestion connectors for popular SaaS applications, databases and file sources, such as Salesforce, Workday, and SQL Server to build incremental data pipelines at scale, fully integrated with Databricks. \n",
    "\n",
    "\n",
    "## 2/ Prepare and transform your data with DLT\n",
    "\n",
    "<div>\n",
    "  <div style=\"width: 45%; float: left; margin-bottom: 10px; padding-right: 45px\">\n",
    "    <p style=\"min-height: 65px;\">\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/diganparikh-dp/Images/refs/heads/main/Icons/LakeFlow%20Connect.jpg\"/> \n",
    "      <strong>Efficient end-to-end ingestion</strong> <br/>\n",
    "      Enable analysts and data engineers to innovate rapidly with simple pipeline development and maintenance \n",
    "    </p>\n",
    "    <p>\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/diganparikh-dp/Images/refs/heads/main/Icons/LakeFlow%20Pipelines.jpg\"/> \n",
    "      <strong>Flexible and easy setup</strong> <br/>\n",
    "      By automating complex administrative tasks and gaining broader visibility into pipeline operations\n",
    "    </p>\n",
    "  </div>\n",
    "  <div style=\"width: 48%; float: left\">\n",
    "    <p style=\"min-height: 65px;\">\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/logo-trust.png\"/> \n",
    "      <strong>Trust your data</strong> <br/>\n",
    "      With built-in orchestration, quality controls and quality monitoring to ensure accurate and useful BI, Data Science, and ML \n",
    "    </p>\n",
    "    <p>\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/logo-stream.png\"/> \n",
    "      <strong>Simplify batch and streaming</strong> <br/>\n",
    "      With self-optimization and auto-scaling data pipelines for batch or streaming processing \n",
    "    </p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fa4037d-b71e-4d55-8a1a-73e632bd5558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Building a DLT pipeline to analyze and reduce churn\n",
    "\n",
    "In this example, we'll implement a end-to-end DLT pipeline consuming our customers information. We'll use the medallion architecture but we could build star schema, data vault or any other modelisation.\n",
    "\n",
    "We'll incrementally load new data with the autoloader, enrich this information and then load a model from MLFlow to perform our customer churn prediction.\n",
    "\n",
    "This information will then be used to build our DBSQL dashboard to track customer behavior and churn.\n",
    "\n",
    "Let's implement the following flow: \n",
    " \n",
    "<div><img width=\"1100px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/retail/lakehouse-churn/lakehouse-retail-churn-de.png?raw=true\"/></div>\n",
    "\n",
    "*Note that we're including the ML model our [Data Scientist built]($../04-Data-Science-ML/04.1-automl-churn-prediction) using Databricks AutoML to predict the churn. We'll cover that in the next section.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b55b094a-7e1f-4dc5-b7e1-0f9157a13a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Your DLT Pipeline has been installed and started for you! Open the <a dbdemos-pipeline-id=\"dlt-churn\" href=\"#joblist/pipelines/bef07be0-ca9f-471f-ab5e-cb73b2a7c024\" target=\"_blank\">Churn DLT pipeline</a> to see it in action.<br/>\n",
    "*(Note: The pipeline will automatically start once the initialization job is completed, this might take a few minutes... Check installation logs for more details)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "8bc0075a-9a55-4fc5-ac08-a8d63bd362db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1/ Loading our data using Databricks Autoloader (cloud_files)\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"500px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-small-1.png\"/>\n",
    "</div>\n",
    "  \n",
    "Autoloader allow us to efficiently ingest millions of files from a cloud storage, and support efficient schema inference and evolution at scale.\n",
    "\n",
    "For more details on autoloader, run `dbdemos.install('auto-loader')`\n",
    "\n",
    "Let's use it to our pipeline and ingest the raw JSON & CSV data being delivered in our blob storage `/demos/retail/churn/...`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a0fa003-0abf-4bf1-883c-14f84b2e3a28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest raw app events stream in incremental mode "
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dlt.create_table(comment=\"Application events and sessions\")\n",
    "@dlt.expect(\"App events correct schema\", \"_rescued_data IS NULL\")\n",
    "def churn_app_events():\n",
    "  return (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"csv\")\n",
    "      .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "      .load(\"/Volumes/data_pioneers/c360/c360/events\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82fcfbbd-3fec-4411-869f-94c826f6a9da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest raw orders from ERP"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.create_table(comment=\"Spending score from raw data\")\n",
    "@dlt.expect(\"Orders correct schema\", \"_rescued_data IS NULL\")\n",
    "def churn_orders_bronze():\n",
    "  return (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"json\")\n",
    "      .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "      .load(\"/Volumes/data_pioneers/c360/c360/orders\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fcec5f7-eea7-4768-8fee-3fad7dbea191",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest raw user data"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.create_table(comment=\"Raw user data coming from json files ingested in incremental with Auto Loader to support schema inference and evolution\")\n",
    "@dlt.expect(\"Users correct schema\", \"_rescued_data IS NULL\")\n",
    "def churn_users_bronze():\n",
    "  return (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"json\")\n",
    "      .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "      .load(\"/Volumes/data_pioneers/c360/c360/users\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "91c571aa-fcbb-4071-b681-998ca5fdb24a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2/ Enforce quality and materialize our tables for Data Analysts\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"500px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-small-2.png\"/>\n",
    "</div>\n",
    "\n",
    "The next layer often call silver is consuming **incremental** data from the bronze one, and cleaning up some information.\n",
    "\n",
    "We're also adding an [expectation](https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-expectations.html) on different field to enforce and track our Data Quality. This will ensure that our dashboard are relevant and easily spot potential errors due to data anomaly.\n",
    "\n",
    "For more advanced DLT capabilities run `dbdemos.install('dlt-loans')` or `dbdemos.install('dlt-cdc')` for CDC/SCDT2 example.\n",
    "\n",
    "These tables are clean and ready to be used by the BI team!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c228f0c-5aa1-4a22-8444-7a10f348f987",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean and anonymise User data"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.create_table(comment=\"User data cleaned and anonymized for analysis.\")\n",
    "@dlt.expect_or_drop(\"user_valid_id\", \"user_id IS NOT NULL\")\n",
    "def churn_users():\n",
    "  return (dlt\n",
    "          .read_stream(\"churn_users_bronze\")\n",
    "          .select(F.col(\"id\").alias(\"user_id\"),\n",
    "                  F.sha1(F.col(\"email\")).alias(\"email\"), \n",
    "                  F.to_timestamp(F.col(\"creation_date\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"creation_date\"), \n",
    "                  F.to_timestamp(F.col(\"last_activity_date\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"last_activity_date\"), \n",
    "                  F.initcap(F.col(\"firstname\")).alias(\"firstname\"), \n",
    "                  F.initcap(F.col(\"lastname\")).alias(\"lastname\"), \n",
    "                  F.col(\"address\"), \n",
    "                  F.col(\"canal\"), \n",
    "                  F.col(\"country\"),\n",
    "                  F.col(\"gender\").cast(\"int\").alias(\"gender\"),\n",
    "                  F.col(\"age_group\").cast(\"int\").alias(\"age_group\"), \n",
    "                  F.col(\"churn\").cast(\"int\").alias(\"churn\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d75f68b-af3d-4374-8582-26d4fb4a2044",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean orders"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.create_table(comment=\"Order data cleaned and anonymized for analysis.\")\n",
    "@dlt.expect_or_drop(\"order_valid_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"order_valid_user_id\", \"user_id IS NOT NULL\")\n",
    "def churn_orders():\n",
    "  return (dlt\n",
    "          .read_stream(\"churn_orders_bronze\")\n",
    "          .select(F.col(\"amount\").cast(\"int\").alias(\"amount\"),\n",
    "                  F.col(\"id\").alias(\"order_id\"),\n",
    "                  F.col(\"user_id\"),\n",
    "                  F.col(\"item_count\").cast(\"int\").alias(\"item_count\"),\n",
    "                  F.to_timestamp(F.col(\"transaction_date\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"creation_date\"))\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "e716a28e-2e06-4607-9c27-783f3e76fc66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3/ Aggregate and join data to create our ML features\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"500px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-small-3.png\"/>\n",
    "</div>\n",
    "\n",
    "We're now ready to create the features required for our Churn prediction.\n",
    "\n",
    "We need to enrich our user dataset with extra information which our model will use to help predicting churn, sucj as:\n",
    "\n",
    "* last command date\n",
    "* number of item bought\n",
    "* number of actions in our website\n",
    "* device used (ios/iphone)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48af8f0d-b40f-4472-b3d7-d54bf68b279b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.create_table(comment=\"Final user table with all information for Analysis / ML\")\n",
    "def churn_features():\n",
    "  churn_app_events_stats_df = (dlt\n",
    "          .read(\"churn_app_events\")\n",
    "          .groupby(\"user_id\")\n",
    "          .agg(F.first(\"platform\").alias(\"platform\"),\n",
    "               F.count('*').alias(\"event_count\"),\n",
    "               F.count_distinct(\"session_id\").alias(\"session_count\"),\n",
    "               F.max(F.to_timestamp(\"date\", \"MM-dd-yyyy HH:mm:ss\")).alias(\"last_event\"))\n",
    "                              )\n",
    "  \n",
    "  churn_orders_stats_df = (dlt\n",
    "          .read(\"churn_orders\")\n",
    "          .groupby(\"user_id\")\n",
    "          .agg(F.count('*').alias(\"order_count\"),\n",
    "               F.sum(\"amount\").alias(\"total_amount\"),\n",
    "               F.sum(\"item_count\").alias(\"total_item\"),\n",
    "               F.max(\"creation_date\").alias(\"last_transaction\"))\n",
    "         )\n",
    "  \n",
    "  return (dlt\n",
    "          .read(\"churn_users\")\n",
    "          .join(churn_app_events_stats_df, on=\"user_id\")\n",
    "          .join(churn_orders_stats_df, on=\"user_id\")\n",
    "          .withColumn(\"days_since_creation\", F.datediff(F.current_timestamp(), F.col(\"creation_date\")))\n",
    "          .withColumn(\"days_since_last_activity\", F.datediff(F.current_timestamp(), F.col(\"last_activity_date\")))\n",
    "          .withColumn(\"days_last_event\", F.datediff(F.current_timestamp(), F.col(\"last_event\")))\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "35e10c4e-79df-4181-be0f-6c5f0a30de5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5/ Enriching the gold data with a ML model\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"500px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-small-4.png\"/>\n",
    "</div>\n",
    "\n",
    "Our Data scientist team has build a churn prediction model using Auto ML and saved it into Databricks Model registry. \n",
    "\n",
    "One of the key value of the Lakehouse is that we can easily load this model and predict our churn right into our pipeline. \n",
    "\n",
    "Note that we don't have to worry about the model framework (sklearn or other), MLFlow abstract that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fff7450f-a2eb-4367-83c0-42c9b8539e66",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load the model as SQL function"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow \n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "#                                                                                                     Stage/version  \n",
    "#                                                                                   Model name               |        \n",
    "#                                                                                       |                    |        \n",
    "predict_churn_udf = mlflow.pyfunc.spark_udf(spark, \"models:/data_pioneers.c360.dbdemos_customer_churn@prod\", \"long\", env_manager='virtualenv')\n",
    "spark.udf.register(\"predict_churn\", predict_churn_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72e94cce-01c1-4dfd-970f-8683ae0be9a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Call our model and predict churn in our pipeline"
    }
   },
   "outputs": [],
   "source": [
    "model_features = predict_churn_udf.metadata.get_input_schema().input_names()\n",
    "\n",
    "@dlt.create_table(comment=\"Customer at risk of churn\")\n",
    "def churn_prediction():\n",
    "  return (dlt\n",
    "          .read('churn_features')\n",
    "          .withColumn('churn_prediction', predict_churn_udf(*model_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e60f4f03-9fb4-43ab-a285-a3268a42a7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Our pipeline is now ready!\n",
    "\n",
    "As you can see, building Data Pipeline with databricks let you focus on your business implementation while the engine solves all hard data engineering work for you.\n",
    "\n",
    "Open the <a dbdemos-pipeline-id=\"dlt-churn\" href=\"#joblist/pipelines/bef07be0-ca9f-471f-ab5e-cb73b2a7c024\" target=\"_blank\">Churn DLT pipeline</a> and click on start to visualize your lineage and consume the new data incrementally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c894863-0422-4b76-9103-a33bd9bb5136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Next: secure and share data with Unity Catalog\n",
    "\n",
    "Now that these tables are available in our Lakehouse, let's review how we can share them with the Data Scientists and Data Analysts teams.\n",
    "\n",
    "Jump to the [Governance with Unity Catalog notebook]($../00-churn-introduction-lakehouse) or [Go back to the introduction]($../00-churn-introduction-lakehouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6335c13-284c-4c8e-a66c-e32f750453e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optional: Checking your data quality metrics with DLT\n",
    "DLT tracks all your data quality metrics. You can leverage the expecations directly as SQL table with Databricks SQL to track your expectation metrics and send alerts as required. This let you build the following dashboards:\n",
    "\n",
    "<img width=\"1000\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/retail/lakehouse-churn/lakehouse-retail-c360-dashboard-dlt-stat.png?raw=true\">\n",
    "\n",
    "<a dbdemos-dashboard-id=\"dlt-quality-stat\" href='/sql/dashboardsv3/01f06ca9586f10459b035cc4a1f29312' target=\"_blank\">Data Quality Dashboard</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "135b112c-70b7-4a0d-936e-0b22bdd5846a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Building our first business dashboard with Databricks SQL\n",
    "\n",
    "Our data is now available! We can start building dashboards to get insights from our past and current business.\n",
    "\n",
    "<img style=\"float: left; margin-right: 50px;\" width=\"500px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/retail/lakehouse-churn/lakehouse-retail-c360-dashboard-churn-prediction.png?raw=true\" />\n",
    "\n",
    "<img width=\"500px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/retail/lakehouse-churn/lakehouse-retail-c360-dashboard-churn.png?raw=true\"/>\n",
    "\n",
    "<a dbdemos-dashboard-id=\"churn-universal\" href='/sql/dashboardsv3/01f06ca9586f10459b035cc4a1f29312'  target=\"_blank\">Open the DBSQL Dashboard</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "01.3-DLT-churn-python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
